{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cd69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../files/my_object.pkl\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "[199, 221, 28, 221, 39, 78, 76, 64, 81, 84, 257, 70, 64, 76, 76, 64, 81, 84, 257, 28, 221, 199, 199, 199, 221, 39, 78, 76, 64, 81, 84, 257, 70, 64, 76, 76, 64, 81, 84, 257, 11, 221, 74, 77, 78, 86, 260, 64, 257, 258, 256, 36, 84, 81, 78, 79, 68, 64, 260, 75, 78, 65, 82, 83, 68, 81, 221, 78, 81, 221, 66, 78, 76, 76, 78, 260, 75, 78, 65, 82, 83, 68, 81, 221, 11, 221, 72, 257, 64, 221, 82, 79, 68, 66, 72, 68, 257, 78, 69, 221, 66, 75, 64, 86, 68, 259, 75, 78, 65, 82, 83, 68, 81, 221, 69, 81, 78, 76, 221, 258, 256, 68, 64, 82, 83, 68, 81, 260, 32, 83, 75, 64, 77, 83, 72, 66, 221, 46, 66, 68, 64, 260, 11, 221, 44, 68, 67, 72, 83, 68, 81, 81, 64, 77, 68, 64, 260, 50, 68, 64, 221, 64, 77, 259, 79, 64, 81, 83, 257, 78, 69, 221, 258, 256, 33, 75, 64, 66, 74, 221, 50, 68, 64, 221, 13, 221, 40, 83, 221, 72, 257, 66, 75, 78, 82, 68, 75, 88, 221, 81, 68, 75, 64, 83, 68, 259, 83, 78, 221, 258, 256, 32, 76, 68, 81, 72, 66, 64, 260, 75, 78, 65, 82, 83, 68, 81, 221, 11, 221, 39, 13, 221, 64, 76, 68, 81, 72, 66, 64, 77, 84, 257, 13, 221, 40, 83, 221, 76, 64, 88, 221, 70, 81, 78, 86, 221, 83, 78, 221, 64, 221, 75, 68, 77, 70, 258, 221, 78, 69, 221, 21, 15, 221, 66, 76, 221, 7, 221, 17, 19, 221, 72, 260, 8, 221, 64, 77, 259, 64, 221, 76, 64, 82, 257, 78, 69, 221, 21, 221, 74, 72, 75, 78, 70, 81, 64, 76, 257, 7, 221, 16, 18, 221, 75, 65, 221, 8, 221, 11, 221, 64, 77, 259, 65, 68, 64, 81, 257, 64, 221, 66, 78, 77, 82, 79, 72, 66, 84, 78, 84, 257, 79, 64, 72, 81, 221, 78, 69, 221, 66, 75, 64, 86, 257, 13, 221, 40, 260, 75, 72, 69, 256, 11, 221, 258, 256, 75, 78, 65, 82, 83, 68, 81, 257, 64, 81, 256, 65, 75, 84, 256, 11, 221, 78, 77, 75, 88, 221, 65, 68, 66, 78, 76, 72, 77, 70, 221, 1, 221, 75, 78, 65, 82, 83, 68, 81, 221, 81, 68, 259, 1, 221, 78, 260, 66, 78, 78, 74, 72, 77, 70, 221, 13, 221, 44, 64, 83, 72, 77, 70, 221, 78, 66, 66, 84, 81, 257, 72, 260, 258, 256, 82, 84, 76, 76, 68, 81, 221, 11, 221, 79, 81, 78, 67, 84, 66, 72, 77, 70, 221, 68, 70, 70, 257, 86, 71, 72, 66, 71, 221, 64, 81, 256, 66, 64, 81, 81, 72, 68, 259, 65, 88, 221, 258, 256, 69, 68, 76, 64, 75, 68, 257, 69, 78, 81, 221, 84, 79, 221, 83, 78, 221, 64, 221, 88, 68, 64, 81, 221, 65, 68, 69, 78, 81, 256, 71, 64, 83, 66, 71, 72, 77, 70, 221, 72, 77, 83, 78, 221, 79, 75, 64, 77, 74, 83, 78, 77, 72, 66, 221, 75, 64, 81, 85, 64, 256, 13, 221, 39, 78, 76, 64, 81, 84, 257, 70, 64, 76, 76, 64, 81, 84, 257, 72, 257, 64, 221, 71, 72, 70, 71, 75, 88, 221, 68, 82, 83, 68, 68, 76, 68, 259, 69, 78, 78, 259, 11, 221, 64, 77, 259, 72, 257, 86, 72, 67, 68, 75, 88, 221, 66, 64, 84, 70, 71, 83, 221, 84, 82, 72, 77, 70, 221, 75, 78, 65, 82, 83, 68, 81, 221, 79, 78, 83, 257, 11, 221, 76, 78, 82, 83, 75, 88, 221, 64, 81, 78, 84, 77, 259, 258, 256, 33, 81, 72, 83, 72, 82, 71, 221, 40, 82, 75, 68, 257, 13, 221, 199, 199, 199, 221, 28, 221, 28, 221, 35, 68, 82, 66, 81, 72, 79, 83, 72, 78, 260, 28, 221, 28, 221, 199, 199, 199, 221, 39, 78, 76, 64, 81, 84, 257, 70, 64, 76, 76, 64, 81, 84, 257, 72, 257, 64, 221, 75, 64, 81, 70, 256, 66, 81, 84, 82, 83, 64, 66, 68, 64, 260, 11, 221, 86, 72, 258, 221, 64, 221, 65, 78, 67, 88, 221, 75, 68, 77, 70, 258, 221, 84, 79, 221, 83, 78, 221, 21, 15, 221, 66, 68, 77, 83, 72, 76, 68, 83, 81, 68, 257, 7, 221, 17, 19, 221, 72, 260, 8, 221, 64, 77, 259, 86, 68, 72, 70, 71, 72, 77, 70, 221, 84, 79, 221, 83, 78, 221, 20, 221, 159, 223, 242, 221, 21, 221, 74, 72, 75, 78, 70, 81, 64, 76, 257, 7, 221, 16, 16, 221, 159, 223, 242, 221, 16, 18, 221, 75, 65, 221, 8, 221, 11, 221, 64, 75, 258, 78, 84, 70, 71, 221, 258, 256, 75, 78, 65, 82, 83, 68, 81, 257, 66, 64, 84, 70, 71, 83, 221, 72, 260, 75, 78, 65, 82, 83, 68, 81, 221, 79, 78, 83, 257, 64, 81, 256, 84, 82, 84, 64, 75, 75, 88, 221, 17, 18, 221, 159, 223, 242, 221, 18, 23, 221, 66, 76, 221, 7, 221, 24, 221, 159, 223, 242, 221, 16, 20, 221, 72, 260, 8, 221, 75, 78, 77, 70, 221, 64, 77, 259, 86, 68, 72, 70, 71, 221, 15, 221, 31, 13, 31]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and save the tokenizer\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/Users/benjawesome/coding/positional-gpt-2\")\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"files\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_dir, \"my_object.pkl\")\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "from data.byte_pair_encoding import BytePairTokenizer\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get our data\n",
    "train_text_list = raw_datasets[\"train\"][\"text\"]\n",
    "val_text_list = raw_datasets[\"validation\"][\"text\"]\n",
    "\n",
    "# turn the data into strings for input\n",
    "full_training_corpus = \"\\n\".join(train_text_list)\n",
    "val_training_corpus = \"\\n\".join(val_text_list)\n",
    "\n",
    "# create tokenizer and train\n",
    "tokenizer = BytePairTokenizer(full_training_corpus[:20000])\n",
    "tokenizer.train(5)\n",
    "\n",
    "# save the trained tokenizer\n",
    "tokenizer_state = {\n",
    "    'merges': tokenizer.merges,\n",
    "    'token_to_id': tokenizer.token_ids,\n",
    "    'byte_to_char': tokenizer.byte_to_char\n",
    "}   \n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer_state, f)\n",
    "\n",
    "# test encoding on val set\n",
    "encoded = tokenizer.encode(val_training_corpus[:1000])\n",
    "\n",
    "print(tokenizer.token_ids.get(\",\"))\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d08d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comma id: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "H\n",
      "H\n",
      "l\n",
      "l\n",
      "l\n",
      "M\n",
      "l\n",
      "H\n",
      "g\n",
      "c\n",
      "r\n",
      "l\n",
      "u\n",
      "l\n",
      "u\n",
      "l\n",
      "c\n",
      "g\n",
      "h\n",
      "v\n",
      "e\n",
      "c\n",
      "l\n",
      "p\n",
      "H\n",
      "c\n",
      "w\n",
      "r\n",
      "l\n",
      "l\n",
      "c\n",
      "l\n",
      "c\n",
      "@\n",
      "None\n",
      "[373, 298, None, 315, 271, 630, 1041, 748, 630, 1649, None, 315, 271, 630, 1041, 748, 2712, 3198, 2684, None, 597, 2833, 312, 1998, None, 597, 290, 588, 919, 1323, 278, 407, 585, 268, None, 597, 2833, 612, 2552, 2835, 1174, 1709, 266, None, 3152, 261, 535, 1709, 2081, 1188, 448, 3777, 3226, 2081, 279, 2296, 1744, 584, 4275, 418, 1113, None, 597, 290, 588, None, 270, 318, 261, 294, 265, 630, 709, 1110, None, 2111, 1159, 2813, 3195, 2262, None, 1861, 2202, 2574, 1455, 3322, 542, 517, 3781, 502, None, 2921, 362, 1811, None, 926, 2655, 358, 271, 1702, 359, 420, 294, None, 618, 4464, 278, 407, 585, 257, 540, 1070, 1200, None, 597, 290, 390, 455, 505, None, 370, 736, 2376, 2135, None, 597, 2833, 1153, 308, 284, None, 419, 830, 844, 885, 2016, 3349, 4674, 588, 852, 286, 670, None, 257, 456, 455, 2514, 1049, 2155, 2873, 351, 706, 1159, 1096, 915, None, 292, 2732, 848, 340, 1814, 3094, 434, 744, None, 3959, 487, 315, 271, 630, 1041, 748, 630, 919, 4311, 504, None, 339, 268, 3492, 457, 301, 4441, None, 4061, 2347, None, 597, 2833, 489, 2051, 3573, 2983, 1163, 1338, 945, 4086, 316, 640, None, 324, 643, None, 315, 271, 630, 1041, 748, 630, 919, 1314, 3364, 290, 313, 1709, 266, 902, 1971, 4965, 2695, 2262, None, 281, 484, 383, 2341, 2202, 285, 1560, None, 2237, 286, 2695, 4831, 517, 3781, 502, None, 2921, 362, 1706, 527, 1811, None, 926, 735, 1498, 263, None, 597, 290, 390, None, 4061, 285, None, 597, 2833, 489, 421, 455, 3108, 2252, 527, 3272, None, 1861, 603, 527, 1477, 2574, 1202, 280, 2644, 3418, 371, 577, None]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and test the tokenizer object\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"/Users/benjawesome/coding/positional-gpt-2\")\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"files\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(save_dir, \"my_object.pkl\")\n",
    "\n",
    "with open(save_path, 'rb') as f:\n",
    "    tokenizer_state = pickle.load(f)\n",
    "\n",
    "token_ids = tokenizer_state.get('token_to_id')\n",
    "\n",
    "from data.byte_pair_encoding import BytePairTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = BytePairTokenizer.load(save_path)\n",
    "\n",
    "print(\"comma id: \" + str(tokenizer.token_ids.get(\",\")))\n",
    "\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# get our data\n",
    "train_text_list = raw_datasets[\"train\"][\"text\"]\n",
    "val_text_list = raw_datasets[\"validation\"][\"text\"]\n",
    "\n",
    "# turn the data into strings for input\n",
    "full_training_corpus = \"\\n\".join(train_text_list)\n",
    "val_training_corpus = \"\\n\".join(val_text_list)\n",
    "\n",
    "print(token_ids.get(\",\"))\n",
    "\n",
    "# realized that my tokenizer didn't handle punctuation correct\n",
    "\n",
    "# fix the missing puncutations\n",
    "\n",
    "MISSING_TOKENS = [\",\", \".\", \"!\", \"?\", \" \"]\n",
    "\n",
    "next_available_id = 5000\n",
    "\n",
    "for token_string in MISSING_TOKENS:\n",
    "    if token_string not in tokenizer.token_ids:\n",
    "        tokenizer.token_ids[token_string] = next_available_id\n",
    "        next_available_id += 1\n",
    "        print(f\"Assigned ID {tokenizer.token_ids[token_string]} to token: '{token_string}'\")\n",
    "\n",
    "tokenizer_state = {\n",
    "    'merges': tokenizer.merges,\n",
    "    'token_to_id': tokenizer.token_ids,\n",
    "    'byte_to_char': tokenizer.byte_to_char\n",
    "}   \n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer_state, f)\n",
    "\n",
    "print(tokenizer.token_ids.get(\",\"))\n",
    "\n",
    "encoded = tokenizer.encode(val_training_corpus[:1000])\n",
    "\n",
    "print(encoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
